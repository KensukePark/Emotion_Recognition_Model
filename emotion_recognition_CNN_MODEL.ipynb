{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install keras"
      ],
      "metadata": {
        "id": "yc9t3v9_A9dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-plot"
      ],
      "metadata": {
        "id": "ZkCcdhBSCexb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6Dv2hZplD6Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scikitplot\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, Activation\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import np_utils"
      ],
      "metadata": {
        "id": "jEEejjADH7MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/content/drive/My Drive/fer2013.csv'\n",
        "df = pd.read_csv(filename)\n",
        "#구글 드라이브에 fer2013 데이터셋을 보관하여 사용함"
      ],
      "metadata": {
        "id": "sZIA4Kh7HhuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.emotion.unique()"
      ],
      "metadata": {
        "id": "PNMAwtQsAs53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_label_to_text = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}"
      ],
      "metadata": {
        "id": "hkZj-gWwAvUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = pyplot.figure(1, (14, 14))\n",
        "\n",
        "k = 0\n",
        "for label in sorted(df.emotion.unique()):\n",
        "    for j in range(7):\n",
        "        px = df[df.emotion==label].pixels.iloc[k]\n",
        "        px = np.array(px.split(' ')).reshape(48, 48).astype('float32')\n",
        "\n",
        "        k += 1\n",
        "        ax = pyplot.subplot(7, 7, k)\n",
        "        ax.imshow(px, cmap='gray')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_title(emotion_label_to_text[label])\n",
        "        pyplot.tight_layout()"
      ],
      "metadata": {
        "id": "vYo-PsIEIJFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INTERESTED_LABELS = [3, 4, 6]\n",
        "df = df[df.emotion.isin(INTERESTED_LABELS)]"
      ],
      "metadata": {
        "id": "GWpaf23YIQKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_array = df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\n",
        "img_array = np.stack(img_array, axis=0)"
      ],
      "metadata": {
        "id": "y2xXxUEEIUlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "img_labels = le.fit_transform(df.emotion)\n",
        "img_labels = np_utils.to_categorical(img_labels)\n",
        "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))"
      ],
      "metadata": {
        "id": "VXxQuKf6IX6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(img_array, img_labels,\n",
        "                                                    shuffle=True, stratify=img_labels,\n",
        "                                                    test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "C77GEAhpIbuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_width = X_train.shape[1]\n",
        "img_height = X_train.shape[2]\n",
        "img_depth = X_train.shape[3]\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "X_train = X_train / 255.\n",
        "X_valid = X_valid / 255."
      ],
      "metadata": {
        "id": "WHLhXJ4ZIhyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_net(optim):\n",
        "    net = Sequential(name='CNN')\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=64, kernel_size=(5,5), input_shape=(img_width, img_height, img_depth),\n",
        "            activation='elu', padding='same', kernel_initializer='he_normal', name='conv2d_1'))\n",
        "    net.add(BatchNormalization(name='batchnorm_1'))\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=64,kernel_size=(5,5),activation='elu',padding='same',\n",
        "            kernel_initializer='he_normal',name='conv2d_2' ) )\n",
        "    net.add(BatchNormalization(name='batchnorm_2'))\n",
        "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_1'))\n",
        "    net.add(Dropout(0.4, name='dropout_1'))\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=128,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d_3'))\n",
        "    net.add(BatchNormalization(name='batchnorm_3'))\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=128,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d_4'))\n",
        "    net.add(BatchNormalization(name='batchnorm_4'))\n",
        "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_2'))\n",
        "    net.add(Dropout(0.4, name='dropout_2'))\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=256,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d_5'))\n",
        "    net.add(BatchNormalization(name='batchnorm_5'))\n",
        "    net.add(\n",
        "        Conv2D(\n",
        "            filters=256, kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d_6'))\n",
        "    net.add(BatchNormalization(name='batchnorm_6'))\n",
        "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_3'))\n",
        "    net.add(Dropout(0.5, name='dropout_3'))\n",
        "    net.add(Flatten(name='flatten'))\n",
        "    net.add(Dense(128, activation='elu',kernel_initializer='he_normal',name='dense_1'))\n",
        "    net.add(BatchNormalization(name='batchnorm_7'))\n",
        "    net.add(Dropout(0.6, name='dropout_4'))\n",
        "    net.add(Dense( num_classes,activation='softmax', name='out_layer') )\n",
        "    net.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['accuracy'] )\n",
        "    net.summary()\n",
        "    return net"
      ],
      "metadata": {
        "id": "lnfxZgX1Il-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    min_delta=0.00005,\n",
        "    patience=11,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,\n",
        "    patience=7,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    early_stopping,\n",
        "    lr_scheduler,\n",
        "]"
      ],
      "metadata": {
        "id": "eWy9aXCcIqbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    shear_range=0.15,\n",
        "    zoom_range=0.15,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "train_datagen.fit(X_train)"
      ],
      "metadata": {
        "id": "1egaYpY7IuY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 \n",
        "epochs = 100\n",
        "optims = [\n",
        "    optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Nadam'),\n",
        "    optimizers.Adam(0.001),\n",
        "]\n",
        "\n",
        "model = build_net(optims[1]) \n",
        "history = model.fit_generator(\n",
        "    train_datagen.flow(X_train, y_train, batch_size=batch_size),\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    steps_per_epoch=len(X_train) / batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    use_multiprocessing=True\n",
        ")"
      ],
      "metadata": {
        "id": "eQWHhSYTIwPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_yaml = model.to_yaml()\n",
        "with open(\"model.yaml\", \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "    \n",
        "model.save(\"model.h5\")\n",
        "\n",
        "sns.set()\n",
        "fig = pyplot.figure(0, (12, 4))\n",
        "\n",
        "ax = pyplot.subplot(1, 2, 1)\n",
        "sns.lineplot(history.epoch, history.history['accuracy'], label='train')\n",
        "sns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.tight_layout()\n",
        "\n",
        "ax = pyplot.subplot(1, 2, 2)\n",
        "sns.lineplot(history.epoch, history.history['loss'], label='train')\n",
        "sns.lineplot(history.epoch, history.history['val_loss'], label='valid')\n",
        "pyplot.title('Loss')\n",
        "pyplot.tight_layout()\n",
        "\n",
        "pyplot.savefig('epoch_history_dcnn.png')\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "RHvf2qCWKh9I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}